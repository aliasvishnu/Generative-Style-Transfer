{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 2: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "from keras.applications import VGG16, vgg16\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (0, 1, 2)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    sLoss = K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "    return sLoss\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "    b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 1.0\n",
    "style_weight = 100.0\n",
    "content_weight = 0.025\n",
    "\n",
    "img_nrows = 400\n",
    "img_ncols = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# style_loss\n",
    "def get_loss(y_true, y_pred):\n",
    "    loss = style_loss(y_true[0, :, :, :], y_pred[0, :, :, :])\n",
    "    print \"loss\", K.shape(loss), loss\n",
    "    return style_weight*loss\n",
    "\n",
    "def c_loss(y_true, y_pred):\n",
    "    loss = content_loss(y_true[0, :, :, :], y_pred[0, :, :, :])\n",
    "    print \"c_loss\", K.shape(loss), loss\n",
    "#     loss += total_variation_loss(y_pred)\n",
    "    return content_weight*loss\n",
    "\n",
    "def var_loss(y_true, y_pred):\n",
    "    loss = total_variation_weight*total_variation_loss(y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine generative layer with VGG layer\n",
    "inputs = Input(shape=(3, 400, 400))\n",
    "\n",
    "vggTmp = vgg16.VGG16(input_tensor = inputs, weights='imagenet', include_top=False)\n",
    "for layer in vggTmp.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "x1 = vggTmp.layers[12].output\n",
    "# 512, 28, 28\n",
    "\n",
    "# Joining layer\n",
    "x = Convolution2D(64, 3, 3, border_mode = 'same', activation = 'tanh', init = 'glorot_normal')(x1)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'tanh', init = 'glorot_normal')(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'tanh', init = 'glorot_normal')(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'tanh', init = 'glorot_normal')(x)\n",
    "\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) # 28, 28 -> 224, 224\n",
    "out = Convolution2D(3, 5, 5, border_mode = 'same', activation = 'sigmoid', init = 'glorot_normal')(x)\n",
    "\n",
    "\n",
    "generative_model = Model(input = inputs, output = out)\n",
    "\n",
    "# VGG model\n",
    "vgg_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "fl = ['block1_conv1', 'block2_conv1','block3_conv1', 'block4_conv1', 'block5_conv1', 'block4_conv2']\n",
    "ot = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n",
    "outputs = [vgg_model.input, ot[fl[5]], ot[fl[0]], ot[fl[1]], ot[fl[2]], ot[fl[3]], ot[fl[4]]]\n",
    "# Discriminative model\n",
    "disc_model = Model(input = vgg_model.input, output = outputs)\n",
    "vgg_out = disc_model(out)\n",
    "\n",
    "# Final model\n",
    "fin_model = Model(input = inputs, output = vgg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_loss Shape.0 Sum{acc_dtype=float64}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "fin_model.compile(loss = [var_loss, c_loss, get_loss, get_loss, get_loss, get_loss, get_loss], optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_style_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    styleY = disc_model.predict([inp/256.0])\n",
    "    return styleY\n",
    "\n",
    "def get_origin_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    contentY = disc_model.predict([inp/256.0])\n",
    "    return contentY\n",
    "\n",
    "def get_train_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    trainX = np.reshape(inp, (1, 3, 400, 400))\n",
    "#     trainX2 = np.reshape(trainX, (1, np.product(trainX.shape)))\n",
    "    trainX3 = trainX/256.0\n",
    "    return trainX3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "styleY = get_style_features('starry400.jpg') \n",
    "contentY = get_origin_features('ucsd400.jpg')\n",
    "trainX3 = get_train_features('ucsd400.jpg')\n",
    "styleY[1] = contentY[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 86.8864820004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "hist = fin_model.fit(trainX3, styleY, nb_epoch = 300, verbose = 0)\n",
    "end = time.time()\n",
    "print \"Time taken\", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = generative_model.predict(get_train_features('sun.jpg'))\n",
    "img = img[0]\n",
    "imgB = img.swapaxes(0, 2).swapaxes(0, 1)\n",
    "print imgB.shape\n",
    "plt.imshow(imgB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here the dataset construction and training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def loadCaltechImages(path, n_samples):\n",
    "    catList = listdir(path)\n",
    "    \n",
    "    data = []\n",
    "    for cat in catList[:100]:\n",
    "        deepPath = path+cat+\"/\"\n",
    "        imageList = listdir(deepPath)\n",
    "        \n",
    "        for images in imageList[0:n_samples]:\n",
    "            img = load_img(deepPath+images, target_size = (400, 400))\n",
    "            data.append(img)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "path = \"/mnt/cube/VGG_/256_ObjectCategories/\"    \n",
    "n_samples = 5\n",
    "imageList = loadCaltechImages(path, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_style_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    styleY = disc_model.predict([inp/256.0])\n",
    "    return styleY\n",
    "\n",
    "def get_origin_features(inp):\n",
    "    inp = inp.swapaxes(0, 2).swapaxes(1, 2)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    contentY = disc_model.predict([inp/256.0])\n",
    "    return contentY\n",
    "\n",
    "def get_train_features(inp):\n",
    "    inp = inp.swapaxes(0, 2).swapaxes(1, 2)\n",
    "    trainX = np.reshape(inp, (1, 3, 400, 400))\n",
    "    trainX3 = trainX/256.0\n",
    "    return trainX3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "styleY = get_style_features('starry400.jpg') \n",
    "for image in imageList:\n",
    "    contentY = get_origin_features(np.asarray(image))\n",
    "    trainX = get_train_features(np.asarray(image))\n",
    "    styleY[1] = contentY[1]\n",
    "    X.append(trainX)\n",
    "    Y.append(styleY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "batch_size = 100\n",
    "n_train = len(X)\n",
    "for i in range(nb_epochs):\n",
    "    batch = np.random.choice(n_train, 100)\n",
    "    for item in batch:\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        fin_model.fit(x, y, nb_epoch = 1, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
