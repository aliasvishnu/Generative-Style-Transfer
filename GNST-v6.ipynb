{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "from keras.applications import VGG16, vgg16\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (0, 1, 2)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    sLoss = K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "    return sLoss\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "    b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 1.0\n",
    "style_weight = 100.0\n",
    "content_weight = 0.025\n",
    "\n",
    "img_nrows = 400\n",
    "img_ncols = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# style_loss\n",
    "def get_loss(y_true, y_pred):\n",
    "    loss = style_loss(y_true[0, :, :, :], y_pred[0, :, :, :])\n",
    "    print \"loss\", K.shape(loss), loss\n",
    "    return style_weight*loss\n",
    "\n",
    "def c_loss(y_true, y_pred):\n",
    "    loss = content_loss(y_true[0, :, :, :], y_pred[0, :, :, :])\n",
    "    print \"c_loss\", K.shape(loss), loss\n",
    "#     loss += total_variation_loss(y_pred)\n",
    "    return content_weight*loss\n",
    "\n",
    "def var_loss(y_true, y_pred):\n",
    "    loss = total_variation_weight*total_variation_loss(y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine generative layer with VGG layer\n",
    "inputs = Input(shape=(3, None,None))\n",
    "\n",
    "vggTmp = vgg16.VGG16(input_tensor = inputs, weights='imagenet', include_top=False)\n",
    "for layer in vggTmp.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "x1 = vggTmp.layers[12].output\n",
    "# 512, 28, 28\n",
    "\n",
    "# Joining layer\n",
    "x = Convolution2D(64, 3, 3, border_mode = 'same', activation = 'relu', init = 'glorot_normal')(x1)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'relu', init = 'glorot_normal')(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'relu', init = 'glorot_normal')(x)\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) #shape is (16, 14, 14)\n",
    "x = Convolution2D(32, 5, 5, border_mode = 'same', activation = 'relu', init = 'glorot_normal')(x)\n",
    "\n",
    "\n",
    "x = UpSampling2D(size = (2, 2))(x) # 28, 28 -> 224, 224\n",
    "out = Convolution2D(3, 5, 5, border_mode = 'same', activation = 'sigmoid', init = 'glorot_normal')(x)\n",
    "\n",
    "\n",
    "generative_model = Model(input = inputs, output = out)\n",
    "\n",
    "# VGG model\n",
    "vgg_model = VGG16(weights = 'imagenet', include_top = False)\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "fl = ['block1_conv1', 'block2_conv1','block3_conv1', 'block4_conv1', 'block5_conv1', 'block4_conv2']\n",
    "ot = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n",
    "outputs = [vgg_model.input, ot[fl[5]], ot[fl[0]], ot[fl[1]], ot[fl[2]], ot[fl[3]], ot[fl[4]]]\n",
    "# Discriminative model\n",
    "disc_model = Model(input = vgg_model.input, output = outputs)\n",
    "vgg_out = disc_model(out)\n",
    "\n",
    "# Final model\n",
    "fin_model = Model(input = inputs, output = vgg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_loss Shape.0 Sum{acc_dtype=float64}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n",
      "loss Shape.0 Elemwise{true_div,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "fin_model.compile(loss = [var_loss, c_loss, get_loss, get_loss, get_loss, get_loss, get_loss], optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_style_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    styleY = disc_model.predict([inp/256.0])\n",
    "    return styleY\n",
    "\n",
    "def get_origin_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    inp = np.reshape(inp, (1, 3, 400, 400))\n",
    "    contentY = disc_model.predict([inp/256.0])\n",
    "    return contentY\n",
    "\n",
    "def get_train_features(filename):\n",
    "    jpgfile = Image.open(filename)\n",
    "    inp = np.array(jpgfile.getdata())\n",
    "    inp = inp.swapaxes(0, 1)\n",
    "    trainX = np.reshape(inp, (1, 3, 400, 400))\n",
    "#     trainX2 = np.reshape(trainX, (1, np.product(trainX.shape)))\n",
    "    trainX3 = trainX/256.0\n",
    "    return trainX3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "styleY = get_style_features('starry400.jpg') \n",
    "contentY = get_origin_features('ucsd400.jpg')\n",
    "trainX3 = get_train_features('ucsd400.jpg')\n",
    "styleY[1] = contentY[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "hist = fin_model.fit(trainX3, styleY, nb_epoch = 300, verbose = 0)\n",
    "end = time.time()\n",
    "print \"Time taken\", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(xrange(0, 300), hist.history['loss'])\n",
    "plt.savefig('Loss_VGNST_relu.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "scipy.misc.imsave('style_VGNST_relu.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = generative_model.predict(trainX3)\n",
    "img = img[0]\n",
    "img = img.swapaxes(0, 2).swapaxes(0, 1)\n",
    "plt.imshow(img)\n",
    "#plt.savefig('img_vgg-gnst_tanh_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-afe0c9725e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerative_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_train_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sun.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimgB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mimgB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9c8c002b0a6d>\u001b[0m in \u001b[0;36mget_train_features\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjpgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#     trainX2 = np.reshape(trainX, (1, np.product(trainX.shape)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrainX3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m256.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "img = generative_model.predict(get_train_features('sun.jpg'))\n",
    "img = img[0]\n",
    "imgB = img.swapaxes(0, 2).swapaxes(0, 1)\n",
    "print imgB.shape\n",
    "plt.imshow(imgB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More layers - 37.43s\n",
    "# Less layers - 32"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
